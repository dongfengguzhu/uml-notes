% Contributors: Udai Nagpal

\section{Gradient Descent in Autoencoders}
\subsection{Introduction}
Practically speaking, autoencoders are often implemented and
optimized as feed-forward neural networks and optimized via gradient
descent. Given the prevalence of gradient descent training for
autoencoders, an important problem is, assuming a certain
data-generating process, how to determine how effective gradient
descent is in recovering the underlying parameters for that
data-generating process \cite{nguyen2019dynamics}.

We assume that data is generated by a generative bilinear model. The
generative bilinear model is defined as $y=Ax^* + \eta$, where $y$ is
the observation, $x^*$ is a latent code vector, $\eta$ is a random,
independent noise vector, and $A$ is the ground truth mapping from
the latent space to the observation. It is assumed that the columns
of $A$ have norm 1. This form accurately characterizes
mixture-of-gaussians, sparse coding, and sparsity models.

This work considers two-layer autoencoder with weight sharing. We
will let $W^T$ be encoder weights and $W$ be decoder weights, let $b$
be the bias vector for the encoder, and assume no bias for the
decoder. Then $x = \vec{\Sigma}(W^T y + b)$ and $\hat{y} = Wx$. The
reconstruction loss can be written as $$L = \frac{1}{2} \left \lVert
y - \hat{y} \right \rVert^2 = \frac{1}{2} \left \lVert y - W
\vec{\Sigma}(W^T y + b) \right \rVert ^2$$ The activation functions
considered will be the ReLU and threshold activation functions, which
are defined as follows: $ReLU(z) = \max(z,0)$ and
$threshold_{\lambda} (z) = z \mathbbm{1}_{|z| > \lambda}$

The main result that will be shown is that (assuming data is
generated by a generative bilinear model) two-layer autoencoders,
trained with (normalized) gradient descent over the reconstruction
loss, provably learn the parameters of the underlying generative
bilinear model.

\subsection{Descent Property}
In order to prove that gradient descent is able, under certain
conditions, to converge to the underlying generative bilinear model
parameters, a definition quantifying nearness is needed. During the
training process, we need a measure for the quality of recovery of
the generative model groundtruth matrix $A$. Note that $A$ can only
be recovered up to the set of columns; since the dimension ordering
in the latent space is ambiguous, the column ordering of $A$ is
ambiguous. Let $\pi$ be an operator that permutes the columns of a
matrix.

\textbf{Definition 1} ($\delta$-closeness and ($\delta,
\xi)$-nearness) A matrix $W$ is said to be $\delta$-close to $A$ if
there exists an operator $\pi$ such that $\left \lVert \pi(W)_i - A_i
\right \rVert \leq \delta$ for all $i$. We say $W$ is
($\delta,\xi$)-near to $A$ if in addition $\left \lVert \pi(W)-A
\right \rVert \leq \xi \left \lVert A \right \rVert$.

Having defined $\delta$-closeness and ($\delta, \xi)$-nearness, the
main theorem, referred to as the Descent Property, can be stated:

\textbf{Theorem 1.2 for Mixture of Gaussians (Descent Property)}
Suppose that at step s the weight $W^s$ is $(\delta^s, 2)$-near to
$A$. There exists an iterative update rule using an approximate
gradient $g^s$: $W^{s+1} = $normalize$(W^s - \zeta g^s$) that
linearly converges to $A$ when given infinitely many fresh samples.
More precisely, there exists some $\tau \in (1/2,1)$ such that,
supposing that $supp(x)=supp(x^*)$*, that learning rate
$\zeta=\Theta(m)$, and that the bias vector satisfies:
\begin{itemize}
    \item b=0 if $x = threshold_{1/2} (W^T y +b)$, OR
    \item $b^{s+1} =b^s / C$ if $x = ReLU(W^T y + b)$ for some
    constant $C>1$
\end{itemize}
Then, $\left \lVert W^{s+1}-A \right \rVert_F ^2 \leq (1-\tau) \left
\lVert W^s - A \right \rVert_F ^2 + O(mn^{-O(1)})$.

(Note that certain conditions have been omitted for brevity.)

\textbf{Proof Sketch:}

Since ReLU and threshold activation functions can be
nondifferentiable, we formulate an approximate gradient. Observe that
the gradient of $ReLU(z)=\mathbbm{1}[z>0]$ and the gradient of
$threshold_{\lambda} (z) = \mathbbm{1}[|z_i|>\lambda]$. Taking the
gradient of the reconstruction loss with respect to the $i$th column
$W_i \in \mathbb{R}^n$ of $W$:
\begin{align*}
    \nabla _{W_i} L &= -\vec{\Sigma}' (W_i ^T y + b_i) [(W_i ^T y +
    b_i )I+yW_i^T][y - Wx] \\
    \nabla_i L & \approx -\mathbbm{1}_{x_i \neq 0} (W_i ^T y I +b_i
    I+yW_i ^T) (y-Wx)
\end{align*}
Let $g_i$ be an update step $g^s$ in expectation over code $x^*$ and
noise $\eta$:
\begin{align*}
    g_i = -\mathbb{E}[\mathbbm{1}_{x_i \neq 0} (W_i ^T y I +b_i
    I+yW_i ^T) (y-Wx)]\\
\end{align*}
Based on a previous theorem proven in the paper, $g_i$ can be
computed explicitly and is of the form:
\begin{align*}
    g_i = -p_i \lambda_i A + p_i (\lambda_i ^2 +2b_i
    \lambda_i+b_i^2)W_i + \gamma
\end{align*}
where $\lambda_i=<W_i ^s , A_i>$ and $\left \lVert \gamma \right
\rVert =O(n^{-w(1)})$. If the biases are set such that $\lambda_i ^2
+2b_i \lambda_i +b_i ^2 \approx \lambda_i$, then:
\begin{align*}
    g_i &= -p_i \lambda_i A + p_i \lambda_i W_i + \gamma \\
    &= p_i \lambda_i (W_i - A) + \gamma
\end{align*}
which is descent in the desired direction. If this is the case, the
descent property can be established as follows:
\begin{align*}
    \left \lVert ((W^s - \zeta g_s)- A_i) \right \rVert^2 \leq (1-\tau) \left \lVert W_i ^s -A_i \right \rVert ^2 + O(n^{-K})
\end{align*}
where $\tau \in (0,1)$. The descent property inequality is shown to
hold more generally when $W$ is $\delta$-close to $A$ and $|(b_i +
\lambda_i)^2 - \lambda_i| \leq 2(1-\lambda_i)$. As $s$ increases,
$\lambda_i = <W_i ^s, A_i> \rightarrow 1$, and therefore we require
$b_i \rightarrow 0$ in order to establish that the descent property
holds. In the case of threshold activation, we have assumed that all
$b_i=0$, therefore the descent property holds. In the case of ReLU
activations, set $b^{s+1} = \sqrt{1-\tau} b^s$ and the biases
approach 0 sufficiently quickly for the descent property to hold (the
full proof establishes this via induction).

\subsection{Empirical Results} 

Experimental results show close convergence to the underlying
generative model in practice, particularly when the initialization of
the autoencoder weights is a small perturbation of the underlying
generative bilinear model weights. These results provide some
empirical support for the Descent Property.

\subsection{Descent Property Limitations}

The Descent Property assumes the nearness of $W$ to $A$. However, in
practice, this is a potentially problematic assumption: if gradient
descent never arrives at parameters $W$ sufficiently close to $A$,
the Descent Property will not hold. The authors argue the
applicability of the descent property in two ways: 1) In practice,
the typical approach of random initialization leads to good results,
and if the parameters $W$ arrive sufficiently close to $A$ then the
Descent Property applies; 2) For more specific generative models,
coarse approximations can serve as an initialization, and if these
initializations of $W$ are sufficiently close to $A$, then the
Descent Property applies.

All in all, considering the strong empirical performance of gradient
descent with random initialization, it may be possible to show that
the reconstruction loss surface can be optimized effectively even
when further than $O(\delta)$ from the ground truth parameters.
However, this is not established by the Descent Property.